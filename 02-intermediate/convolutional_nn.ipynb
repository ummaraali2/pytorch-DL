{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mz5swP7_kMUD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "1TrvjZhDkzDT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "num_epochs = 5\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "xlKRmRxXk06n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNMtADutk3Lq",
        "outputId": "174cccc1-f850-4494-9266-9d251de8c1e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 67506188.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 43921571.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 29985264.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3520053.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "first layer performs convolution to extract features, typically smaller features to recognize detailed-patterns sort of,\n",
        "\n",
        "applies batch normalization for stable training, introduces non-linearity using ReLU activation, and downsamples through max-pooling.\n",
        "\n",
        "downsampling means taking the maximum value of the feature map because the maximum value in each window represents the most activated or significant feature in that region, ensuring that important features are retained.\n",
        "\n",
        "The second layer continues like the first layer but with more number of filters that kind of combines the patterns recognized in the first layer and tries to output new bigger patterns..and performs furthur more stablization, ReLU activation, and downsampling.\n",
        "\n",
        "Finally, the third layer, the fully connected layer connects the flattened output from the convolutional layers to the specified number of output classes, enabling the network to make predictions and classify input images based on the learned features."
      ],
      "metadata": {
        "id": "sVHaBHTxCmOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional neural network (two convolutional layers)\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(7*7*32, num_classes)\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = ConvNet(num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)"
      ],
      "metadata": {
        "id": "Dj_Cp58xk7L9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNfOzl14lNb-",
        "outputId": "b0f83119-60cb-49ab-8d6a-332ff3761fd8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.0601\n",
            "Epoch [1/5], Step [200/600], Loss: 0.0812\n",
            "Epoch [1/5], Step [300/600], Loss: 0.1294\n",
            "Epoch [1/5], Step [400/600], Loss: 0.0247\n",
            "Epoch [1/5], Step [500/600], Loss: 0.0191\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0081\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0373\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0642\n",
            "Epoch [2/5], Step [300/600], Loss: 0.1292\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0399\n",
            "Epoch [2/5], Step [500/600], Loss: 0.1057\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0336\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0289\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0195\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0164\n",
            "Epoch [3/5], Step [400/600], Loss: 0.1026\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0102\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0617\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0540\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0243\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0178\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0136\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0340\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0337\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0142\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0079\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0600\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0288\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0456\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "metadata": {
        "id": "2NySiH2UlTQN"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}