{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbaFUYv1u8tfbPISho59iT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ummaraali2/pytorch-DL/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S6bBpmFyB0rV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    d = 512  #embedding data\n",
        "    heads = 8\n",
        "    dff = 2048  ##expansion dim\n",
        "    N = 6.  #layers\n",
        "    p = 0.1 #dropout rate\n",
        "    device : str = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    src = torch.randint(0, 100, (4,d))\n",
        "    trg = torch.randint(0, 50, (2,d))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ma_ZBtsBB5Rk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    \" Embedding layer with scalling and dropout. \"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d : int,\n",
        "        vocab_size : int,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "        self.embedding = nn.Embedding(vocab_size, d)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.embedding(x) * math.sqrt(self.d)"
      ],
      "metadata": {
        "id": "8HcqRGxmB-HP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = Embedding(d, 100)\n",
        "e(src).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCqiy241CA9S",
        "outputId": "0496aed6-ada8-4428-830a-1de170b4f5e8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 512, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PE(nn.Module):\n",
        "    \"The Positional Encoding Module with dropout layer. \"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d : int,\n",
        "        p : float,\n",
        "        max_len = 5_000\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pe = torch.zeros(max_len, d)\n",
        "        pos = torch.arange(0, max_len, 1).unsqueeze(1)\n",
        "        div = torch.exp(- torch.arange(0, d, 2)* math.log(10000) / d)\n",
        "        self.pe[:, 0::2] = torch.sin(pos / div)\n",
        "        self.pe[:, 1::2] = torch.cos(pos / div)\n",
        "\n",
        "        self.pe = self.pe.unsqueeze(1)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.register_buffer('pos_embedding', self.pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \" x: shape : [seq_len, batch] \"\n",
        "        return self.dropout(x + self.pe[:x.shape[0], :, :])\n"
      ],
      "metadata": {
        "id": "qVNroklgCpvE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PE(d,p)\n",
        "pe(e(src)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHaAuG3CEOtF",
        "outputId": "64417d7e-7d42-4d2c-ecfe-a7649049c8b5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 512, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \" Multi head self-attention sub-layer followed by Add&Norm layer. \"\n",
        "    def __init__(\n",
        "        self,\n",
        "        heads : int,\n",
        "        d : int,\n",
        "        p : float\n",
        "        ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = heads\n",
        "        self.head_dim = d // heads\n",
        "        self.d = d\n",
        "\n",
        "        self.Q = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.K = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.V = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "\n",
        "        self.linear = nn.Linear(self.d, self.d, bias=False)\n",
        "        self.norm = nn.LayerNorm(d)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask=None) -> Tensor:\n",
        "        batch = q.shape[1]\n",
        "        q_len = q.shape[0]\n",
        "        k_len = k.shape[0]\n",
        "        v_len = v.shape[0]\n",
        "\n",
        "        Q = self.Q(q.reshape(batch, q_len, self.heads, self.head_dim))\n",
        "        K = self.K(k.reshape(batch, k_len, self.heads, self.head_dim))\n",
        "        V = self.V(v.reshape(batch, v_len, self.heads, self.head_dim))\n",
        "        QK = torch.einsum(\"bqhd, bkhd -> bhqk\", [Q, K])\n",
        "\n",
        "\n",
        "        if mask is not None:\n",
        "            QK = QK.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        scale = QK / math.sqrt(self.d)\n",
        "        softmax = self.dropout(F.softmax(scale, dim=-1))\n",
        "        output = torch.einsum(\"bhqk, bvhd -> bqhd\", [softmax, V])\n",
        "        concat = output.reshape(q_len, batch, self.d)\n",
        "        return self.linear(concat)\n",
        ""
      ],
      "metadata": {
        "id": "U6_xmU_bIVCm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = SelfAttention(heads, d, p)\n",
        "x = pe(e(src))\n",
        "s(x,x,x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7apWz8sInKI",
        "outputId": "bc85eade-6700-455a-c82e-6fce18e25d4c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 512, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \" Position-wise fully conntected feed-forward network with 2 linear transformations, where first is followed by GELU(inspiration from GPT) activation with Add&Norm operation.\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d : int,\n",
        "        dff : int,\n",
        "        p : float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d, dff),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dff, d),\n",
        "            nn.Dropout(p)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.ff(x)"
      ],
      "metadata": {
        "id": "hC0Iiu6EI9Y1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder layer with two sub-layers multi-head attention and position-wise fully conntected feed-forward network. \"\n",
        "    def __init__(\n",
        "        self,\n",
        "        heads : int,\n",
        "        d : int,\n",
        "        dff : int,\n",
        "        p : float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = SelfAttention(heads, d, p)\n",
        "        self.ff = FeedForward(d, dff, p)\n",
        "        self.l1 = nn.LayerNorm(d)\n",
        "        self.l2 = nn.LayerNorm(d)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        att_out = self.attention(x, x, x, src_mask)\n",
        "        addnorm1 = self.l1(x + self.dropout(att_out))\n",
        "        ff_out = self.ff(addnorm1)\n",
        "        addnorm2 = self.l2(addnorm1 + self.dropout(ff_out))\n",
        "        return addnorm2"
      ],
      "metadata": {
        "id": "oSoon3zqJ-D7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \" Encoder with N-Encoding layers. \"\n",
        "    def __init__(\n",
        "        self,\n",
        "        N : int,\n",
        "        heads : int,\n",
        "        d : int,\n",
        "        dff : int,\n",
        "        p : float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(heads, d, dff, p) for _ in range(N)])\n",
        "\n",
        "    def forward(self, x: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        for enc_layer in self.encoder:\n",
        "            x = enc_layer(x, src_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hJYJlnoVKSRp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder layer with three sub-layers, two multi-head attention module and position-wise fully conntected feed-forward network on the top.\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        heads : int,\n",
        "        d : int,\n",
        "        dff : int,\n",
        "        p : float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.masked_attention = SelfAttention(heads, d, p)\n",
        "        self.enc_dec_attention = SelfAttention(heads, d, p)\n",
        "        self.ff = FeedForward(d, dff, p)\n",
        "\n",
        "        self.l1 = nn.LayerNorm(d)\n",
        "        self.l2 = nn.LayerNorm(d)\n",
        "        self.l3 = nn.LayerNorm(d)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x: Tensor, k: Tensor, v: Tensor, trg_mask: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        q = self.masked_attention(x, x, x, trg_mask)\n",
        "        addnorm1 = self.l1(self.dropout(q) + x)\n",
        "        enc_dec_out = self.enc_dec_attention(addnorm1, k, v, src_mask)\n",
        "        addnorm2 = self.l2(self.dropout(enc_dec_out) + addnorm1)\n",
        "        ff_out = self.ff(addnorm2)\n",
        "        addnorm3 = self.l3(self.dropout(ff_out) + addnorm2)\n",
        "        return addnorm3"
      ],
      "metadata": {
        "id": "MVJIzagsKakl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \" Decoder with N-Encoding layers. \"\n",
        "    def __init__(\n",
        "        self,\n",
        "        N : int,\n",
        "        heads : int,\n",
        "        d : int,\n",
        "        dff : int,\n",
        "        p : float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(heads, d, dff, p) for _ in range(N)])\n",
        "\n",
        "    def forward(self, x: Tensor, k: Tensor, v: Tensor, trg_mask: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        for dec_layer in self.decoder:\n",
        "            x = dec_layer(x, k, v, trg_mask, src_mask)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "hHeErKPMKe9p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config,\n",
        "        src_vocab_size : int,\n",
        "        trg_vocab_size : int,\n",
        "        src_pad : int,\n",
        "        trg_pad : int\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.src_emb = Embedding(config.d, src_vocab_size)\n",
        "        self.trg_emb = Embedding(config.d, trg_vocab_size)\n",
        "        self.pos_emb = PE(config.d, config.p)\n",
        "        self.encoder = Encoder(config.N, config.heads, config.d, config.dff, config.p)\n",
        "        self.decoder = Decoder(config.N, config.heads, config.d, config.dff, config.p)\n",
        "        self.head = nn.Linear(config.d, trg_vocab_size, bias=False)\n",
        "\n",
        "        self.src_pad = src_pad\n",
        "        self.trg_pad = trg_pad\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor) -> Tensor:\n",
        "        src_mask = self.get_src_mask(src, self.src_pad)\n",
        "        trg_mask = self.get_trg_mask(trg)\n",
        "        src = self.pos_emb(self.src_emb(src))\n",
        "        trg = self.pos_emb(self.trg_emb(trg))\n",
        "        # src and trg shape: [seq_len, batch, d(embedding_size)]\n",
        "        enc_out = self.encoder(src, src_mask)\n",
        "        dec_out = self.decoder(trg, enc_out, enc_out, trg_mask, src_mask)\n",
        "        # dec_out shape: [seq_len, batch, d]\n",
        "        return self.head(dec_out)\n",
        "\n",
        "    def encode(self, src: Tensor):\n",
        "        src_mask = self.get_src_mask(src, self.src_pad)\n",
        "        return self.encoder(self.pos_emb(self.src_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, trg: Tensor, enc_out: Tensor, src_mask: Tensor):\n",
        "        trg_mask = self.get_trg_mask(trg, self.trg_pad)\n",
        "        return self.decoder(self.pos_emb(self.trg_emb(trg)), enc_out, enc_out, trg_mask, src_mask)\n",
        "\n",
        "    def get_src_mask(self, src, pad_idx):\n",
        "        # src shape: [seq_len, batch]\n",
        "        seq_len, batch = src.shape\n",
        "        src_mask = src != pad_idx\n",
        "        return src_mask.reshape(batch, 1, 1, seq_len).to(self.config.device)\n",
        "\n",
        "    def get_trg_mask(self, trg):\n",
        "        seq_len, batch = trg.shape\n",
        "        trg_mask = torch.triu(torch.zeros((seq_len, seq_len))==1).expand(batch, 1, seq_len, seq_len).to(self.config.device)\n",
        "        return trg_mask\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    src = torch.randint(0, 100, (4, 1))\n",
        "    trg = torch.randint(0, 50, (2, 1))\n",
        "    t = Transformer(\n",
        "        TransformerConfig,\n",
        "        src_vocab_size = 100,\n",
        "        trg_vocab_size = 50,\n",
        "        src_pad = 1,\n",
        "        trg_pad = 1\n",
        "        )\n",
        "\n",
        "    print(t(src, trg).shape)\n",
        "    print(torch.backends.mps.is_available())\n",
        "    print(torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj_C9vsUKjth",
        "outputId": "f6ac1963-e0c2-4bd6-fd57-b9078fc05de8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 50])\n",
            "False\n",
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Un50Z-2_KxRs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}